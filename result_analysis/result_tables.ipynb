{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "from texttable import Texttable\n",
    "import latextable\n",
    "\n",
    "from metrics import print_and_analysis_performance_mean_std, print_performance_mean_std, print_overall_performance_mean_std, print_ablation_performance_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8487b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.5\n",
    "alpha = 1\n",
    "seeds = [10, 20, 30, 40, 50]\n",
    "upset_choices = ['upset simple', 'upset ratio', 'upset naive']\n",
    "selected_metrics = ['kendall tau'] + upset_choices\n",
    "NUM_UPSET_CHOICES = len(upset_choices)\n",
    "METRICS_NUM = len(selected_metrics)\n",
    "baselines = ['SpringRank','syncRank','serialRank','btl', 'davidScore',\n",
    "        'eigenvectorCentrality', 'PageRank', 'rankCentrality', 'SVD_RS', 'SVD_NRS']\n",
    "NUM_BASELINES = len(baselines)\n",
    "GNN_selection_choices = ['lr', 'train with', 'pretrain with', 'upset margin coeff', \\\n",
    "                'trainable alpha', 'baseline', \\\n",
    "                         'upset ratio coeff', 'Fiedler layer num', 'pretrain epochs']\n",
    "GNN_selection_choices_curr = ['train with', 'pretrain with', 'upset margin coeff',  \\\n",
    "                 'baseline', 'upset ratio coeff']\n",
    "GNN_CHOICES_NUM = len(GNN_selection_choices)\n",
    "GNN_CHOICES_NUM_CURR = len(GNN_selection_choices_curr)\n",
    "mvr = ['mvr']\n",
    "all_GNNs = ['DIGRAC', 'ib']\n",
    "desirable_list = [1, 2, 5]\n",
    "train_with_list = ['dist', 'innerproduct', 'proximal_dist', 'proximal_innerproduct','proximal_baseline']\n",
    "\n",
    "GNN_variant_names = train_with_list\n",
    "GNN_NUM = 10\n",
    "\n",
    "def generate_method_str_and_compare_names_all(all_methods=baselines):\n",
    "    method_str = ''\n",
    "    for method_name in all_methods:\n",
    "        method_str += method_name\n",
    "    compare_names_all = []\n",
    "    for method_name in all_methods:\n",
    "        if method_name not in ['DIGRAC', 'ib']:\n",
    "            compare_names_all.append(method_name)\n",
    "        else:\n",
    "            for GNN_type in GNN_variant_names:\n",
    "                compare_names_all.append(method_name+'_'+GNN_type)\n",
    "    return method_str, compare_names_all\n",
    "\n",
    "methods_of_interest = ['Non_proximal', 'Proximal']\n",
    "        \n",
    "GNN_names = []\n",
    "for method_name in ['DIGRAC', 'ib']:\n",
    "    for GNN_type in GNN_variant_names:\n",
    "        GNN_names.append(method_name+'_'+GNN_type)\n",
    "\n",
    "non_proximal_ind = [0, 1, 5, 6]\n",
    "proximal_ind = [2, 3, 4, 7, 8, 9]\n",
    "NON_PROXIMAL_GNN_NUM = len(non_proximal_ind)\n",
    "PROXIMAL_GNN_NUM = len(proximal_ind)\n",
    "non_proximal_bool = np.zeros(GNN_NUM, dtype=bool)\n",
    "proximal_bool = np.zeros(GNN_NUM, dtype=bool)\n",
    "for i in non_proximal_ind:\n",
    "    non_proximal_bool[i] = True\n",
    "for i in proximal_ind:\n",
    "    proximal_bool[i] = True\n",
    "\n",
    "GNN_names_non_proximal = ['DIGRAC_dist', 'DIGRAC_innerproduct',\\\n",
    "                          'ib_dist', 'ib_innerproduct']\n",
    "GNN_names_proximal = ['DIGRAC_proximal_dist', 'DIGRAC_proximal_innerproduct', 'DIGRAC_proximal_baseline', \\\n",
    "                      'ib_proximal_dist', 'ib_proximal_innerproduct', 'ib_proximal_baseline']\n",
    "\n",
    "\n",
    "compare_names_all = baselines + mvr + methods_of_interest\n",
    "METHODS_NUM = len(compare_names_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9905e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['DIGRAC_dist', 'DIGRAC_innerproduct',\\\n",
    "                          'ib_dist', 'ib_innerproduct', \\\n",
    "        'DIGRAC_proximal_dist', 'DIGRAC_proximal_innerproduct', 'DIGRAC_proximal_baseline', \\\n",
    "                      'ib_proximal_dist', 'ib_proximal_innerproduct', 'ib_proximal_baseline', \\\n",
    "       ]\n",
    "values = ['DIGRAC dist', 'DIGRAC innerproduct',\\\n",
    "                          'ib dist', 'ib innerproduct', \\\n",
    "        'DIGRAC proximal dist', 'DIGRAC proximal innerproduct', 'DIGRAC proximal baseline', \\\n",
    "                      'ib proximal dist', 'ib proximal innerproduct', 'ib proximal baseline']\n",
    "\n",
    "keys += train_with_list + ['dist', 'innerproduct', 'serial_similarity'] + \\\n",
    "['avg_football', 'avg_finer_football', 'avg_basketball', 'avg_finer_basketball'] + \\\n",
    "['serialRank', 'SpringRank','syncRank','btl', 'eigenvectorCentrality', 'PageRank', 'SVD_NRS']\n",
    "values += ['dist', 'innerproduct', 'proximal dist', 'proximal innerproduct','proximal baseline'] + \\\n",
    "['dist', 'innerproduct', 'SerialRank similarity'] + \\\n",
    "['{\\it Football (avg)}', '{\\it Football finer (avg)}', '{\\it Basketball (avg)}', '{\\it Basketball finer (avg)}'] + \\\n",
    "['SerialRank', 'SpringRank','SyncRank','BTL','Eign.Cent.', 'PageRank', 'SVD\\_NRS']\n",
    "name_mapping_dict = dict(zip(keys, values))\n",
    "print(name_mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list_dict = {}\n",
    "eta_list_dict = {}\n",
    "ERO_style_list_dict = {}\n",
    "K_list_dict = {}\n",
    "season_list_dict = {}\n",
    "season_list_dict['basketball'] = np.arange(1985, 2015)\n",
    "season_list_dict['finer_basketball'] = np.arange(1985, 2015)\n",
    "season_list_dict['football'] = np.arange(2009, 2015)\n",
    "season_list_dict['finer_football'] = np.arange(2009, 2015)\n",
    "for dataset in ['finance','animal', 'faculty_business', 'faculty_cs', 'faculty_history', 'HeadToHead', 'ERO']:\n",
    "    season_list_dict[dataset] = [2009]\n",
    "for dataset in ['basketball', 'finer_basketball', 'football', 'finer_football', 'finance','animal', 'faculty_business', 'faculty_cs', 'faculty_history', 'HeadToHead']:\n",
    "    p_list_dict[dataset] = [0.05]\n",
    "    eta_list_dict[dataset] = [0]\n",
    "    K_list_dict[dataset] = [5]\n",
    "    ERO_style_list_dict[dataset] = ['uniform']\n",
    "\n",
    "p_list_dict['ERO'] = [0.05, 1]\n",
    "eta_list_dict['ERO'] = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "K_list_dict['ERO'] = [5]\n",
    "ERO_style_list_dict['ERO'] = ['uniform', 'gamma']\n",
    "pretrain_with_list_dict = {}\n",
    "for train_with in train_with_list:\n",
    "    if train_with[:8] == 'proximal':\n",
    "        pretrain_with_list_dict[train_with] = ['dist', 'innerproduct', 'serial_similarity']\n",
    "    else:\n",
    "        pretrain_with_list_dict[train_with] = ['dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5458af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_save_name(dataset='HeadToHead', all_methods=all_GNNs, K=5, train_with='dist', upset_ratio_coeff=1.0, upset_margin_coeff=0, upset_margin=0.01,\n",
    "                           trainable_alpha=False, lr=0.01, hidden=32, num_trials=10, train_ratio=1, test_ratio=1,  AllTrain=True, rank_baseline='SpringRank', sigma=1.0, \n",
    "                           Fiedler_layer_num=5, pretrain_epochs=50, pretrain_with='dist'):\n",
    "    default_name_base = ''\n",
    "    if 'DIGRAC' in all_methods or 'ib' in all_methods:\n",
    "        default_name_base += 'K' + str(K) + 'dropout' + str(int(100*dropout))\n",
    "        default_name_base += 'ratio_coe' + str(int(100*upset_ratio_coeff)) + 'margin_coe' + str(int(100*upset_margin_coeff)) \n",
    "        if upset_margin_coeff > 0:\n",
    "            default_name_base += 'margin' + str(int(100*upset_margin)) \n",
    "        default_name_base += 'with' + str(train_with)  + 'Fiedler' + str(Fiedler_layer_num) + 'sigma' + str(int(100*sigma))\n",
    "        default_name_base += 'alpha' + str(int(100*alpha))\n",
    "        if train_with[:8] == 'proximal':\n",
    "            default_name_base += 'train_alpha' + str(trainable_alpha)\n",
    "        default_name_base += 'hid' + str(hidden) + 'lr' + str(int(1000*lr))\n",
    "        default_name_base += 'use' + str(rank_baseline)\n",
    "        if pretrain_epochs > 0 and train_with[:8] == 'proximal':\n",
    "            default_name_base +=  'pre' + str(pretrain_with) + str(int(pretrain_epochs))\n",
    "    save_name_base = default_name_base\n",
    "\n",
    "    default_name_base +=  'trials' + str(num_trials) + 'train_r' + str(int(100*train_ratio)) + 'test_r' + str(int(100*test_ratio)) + 'All' + str(AllTrain)\n",
    "    if dataset[:3] == 'ERO':\n",
    "        default_name_base += 'seeds' + '_'.join([str(value) for value in np.array(seeds).flatten()])\n",
    "    return default_name_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495fc701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_and_K(dataset, season=2009, K=5, p=0.05, ERO_style='uniform', eta=0.1, N=350):\n",
    "    F_style = 'path'\n",
    "    sp_style = 'random'\n",
    "    ambient = 0\n",
    "    if dataset[-1]!='/':\n",
    "        dataset += '/'\n",
    "\n",
    "    if dataset[:3] == 'ERO':\n",
    "        hidden = 8\n",
    "        K = 5\n",
    "        F = 3\n",
    "        default_name_base = 'p' + str(int(100*p)) + 'K' + str(K) + 'N' + str(N)\n",
    "        default_name_base += 'eta' + str(int(100*eta)) + 'style' + str(ERO_style)\n",
    "        dataset = 'ERO/' + default_name_base\n",
    "        dataset_print = 'ERO(p={}, style={},$\\eta$={})'.format(p, ERO_style, eta)\n",
    "    elif dataset[:10].lower() == 'basketball':\n",
    "        hidden = 8\n",
    "        hidden_compare = 8\n",
    "        F = 70\n",
    "        K = 20\n",
    "        dataset = 'Basketball_temporal/' + str(season)\n",
    "        dataset_print = 'Basketball({})'.format(season)\n",
    "    elif dataset[:16].lower() == 'finer_basketball':\n",
    "        hidden = 8\n",
    "        hidden_compare = 8\n",
    "        F = 2\n",
    "        K = 20\n",
    "        dataset = 'Basketball_temporal/finer' + str(season)\n",
    "        dataset_print = 'Basketball finer({})'.format(season)\n",
    "    elif dataset[:6].lower() == 'animal':\n",
    "        hidden = 4\n",
    "        hidden_compare = 4\n",
    "        F = 3\n",
    "        K = 3\n",
    "        dataset = 'Dryad_animal_society/'\n",
    "        dataset_print = 'Animal'\n",
    "    elif dataset[:7].lower() == 'finance':\n",
    "        hidden = 32\n",
    "        hidden_compare = 32\n",
    "        F = 5 # threshold: > 0.7, others have threshold > 0.9\n",
    "        K = 20\n",
    "        dataset_print = 'Finance'\n",
    "    elif dataset[:10].lower() == 'headtohead':\n",
    "        hidden = 16\n",
    "        hidden_compare = 16\n",
    "        F = 39\n",
    "        K = 48\n",
    "        dataset = 'Halo2BetaData/HeadToHead'\n",
    "        dataset_print = 'HeadToHead'\n",
    "    elif dataset[:16].lower() == 'faculty_business':\n",
    "        hidden = 8\n",
    "        hidden_compare = 8\n",
    "        F = 6\n",
    "        K = 5\n",
    "        dataset = 'FacultyHiringNetworks/Business/Business_FM_Full_'\n",
    "        dataset_print = 'Faculty: Business'\n",
    "    elif dataset[:10].lower() == 'faculty_cs':\n",
    "        hidden = 8\n",
    "        hidden_compare = 8\n",
    "        F = 8\n",
    "        K = 9\n",
    "        dataset = 'FacultyHiringNetworks/ComputerScience/ComputerScience_FM_Full_'\n",
    "        dataset_print = 'Faculty: CS'\n",
    "    elif dataset[:15].lower() == 'faculty_history':\n",
    "        hidden = 8\n",
    "        hidden_compare = 8\n",
    "        F = 22\n",
    "        K = 12\n",
    "        dataset = 'FacultyHiringNetworks/History/History_FM_Full_'\n",
    "        dataset_print = 'Faculty: History'\n",
    "    elif dataset[:8].lower() == 'football':\n",
    "        hidden = 4\n",
    "        hidden_compare = 4\n",
    "        F = 19\n",
    "        K = 9\n",
    "        dataset = 'Football_data_England_Premier_League/England_' + str(season) + '_' + str(season+1)\n",
    "        dataset_print = 'Football({})'.format(season)\n",
    "    elif dataset[:14].lower() == 'finer_football':\n",
    "        hidden = 4\n",
    "        hidden_compare = 4\n",
    "        F = 4\n",
    "        K = 9\n",
    "        dataset = 'Football_data_England_Premier_League/finerEngland_' + str(season) + '_' + str(season+1)\n",
    "        dataset_print = 'Football finer({})'.format(season)\n",
    "    return '{\\it '+dataset_print+'}', dataset, K, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [0.01]\n",
    "upset_margin_coeff_list = [0, 1]\n",
    "trainable_alpha_list = [True]\n",
    "rank_baseline_list = ['serialRank', 'SpringRank','syncRank','btl', \n",
    "        'eigenvectorCentrality', 'PageRank', 'SVD_NRS']\n",
    "upset_ratio_coeff_list = [0, 1]\n",
    "Fiedler_layer_num_list = [5]\n",
    "pretrain_epochs_list = [50]\n",
    "non_proximal_ind_correspondence_dict = {}\n",
    "proximal_ind_correspondence_dict = {}\n",
    "i = 0\n",
    "for lr_ind, lr in enumerate(lr_list):\n",
    "    for train_ind, train_with in enumerate(train_with_list):\n",
    "        for pretrain_ind, pretrain_with in enumerate(pretrain_with_list_dict[train_with]):\n",
    "            for margin_coeff_ind, upset_margin_coeff in enumerate(upset_margin_coeff_list):\n",
    "                for trainable_alpha_ind, trainable_alpha in enumerate(trainable_alpha_list):\n",
    "                    for base_ind, rank_baseline in enumerate(rank_baseline_list):\n",
    "                        for ratio_coeff_ind, upset_ratio_coeff in enumerate(upset_ratio_coeff_list):\n",
    "                            for Fiedler_layer_num_ind, Fiedler_layer_num in enumerate(Fiedler_layer_num_list):\n",
    "                                for pretrain_epochs_ind, pretrain_epochs in enumerate(pretrain_epochs_list):\n",
    "                                    for method_ind in range(NON_PROXIMAL_GNN_NUM):\n",
    "                                        non_proximal_ind_correspondence_dict[i] = [lr_ind, train_ind, pretrain_ind, \\\n",
    "                                                                        margin_coeff_ind, trainable_alpha_ind, \\\n",
    "                                                                        base_ind, ratio_coeff_ind, \\\n",
    "                                                                    Fiedler_layer_num_ind, pretrain_epochs_ind, method_ind]\n",
    "                                        i += 1\n",
    "i = 0\n",
    "for lr_ind, lr in enumerate(lr_list):\n",
    "    for train_ind, train_with in enumerate(train_with_list):\n",
    "        for pretrain_ind, pretrain_with in enumerate(pretrain_with_list_dict[train_with]):\n",
    "            for margin_coeff_ind, upset_margin_coeff in enumerate(upset_margin_coeff_list):\n",
    "                for trainable_alpha_ind, trainable_alpha in enumerate(trainable_alpha_list):\n",
    "                    for base_ind, rank_baseline in enumerate(rank_baseline_list):\n",
    "                        for ratio_coeff_ind, upset_ratio_coeff in enumerate(upset_ratio_coeff_list):\n",
    "                            for Fiedler_layer_num_ind, Fiedler_layer_num in enumerate(Fiedler_layer_num_list):\n",
    "                                for pretrain_epochs_ind, pretrain_epochs in enumerate(pretrain_epochs_list):\n",
    "                                    for method_ind in range(PROXIMAL_GNN_NUM):\n",
    "                                        proximal_ind_correspondence_dict[i] = [lr_ind, train_ind, pretrain_ind, \\\n",
    "                                                                        margin_coeff_ind, trainable_alpha_ind, \\\n",
    "                                                                        base_ind, ratio_coeff_ind, \\\n",
    "                                                                    Fiedler_layer_num_ind, pretrain_epochs_ind, method_ind]\n",
    "                                        i += 1\n",
    "non_proximal_cases_num = len(non_proximal_ind_correspondence_dict.keys())\n",
    "proximal_cases_num = len(proximal_ind_correspondence_dict.keys())\n",
    "print(non_proximal_cases_num, proximal_cases_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_results(dataset, season=2009, K=5, upset_ratio_coeff=1.0, upset_margin=0.01, p=0.1, \n",
    "                       AllTrain=True, eta=0.1, lr=0.05, hidden=32, \n",
    "                        N=350, ERO_style='uniform', train_ratio = 0.8, test_ratio = 0.1, dropout=0.5, sigma=1.0, \n",
    "                           methods_of_interest=['Non_proximal', 'Proximal'], print_latex=True):\n",
    "    F_style = 'path'\n",
    "    sp_style = 'random'\n",
    "    num_trials = 2\n",
    "    seed = 31\n",
    "    fill_val = 0.5\n",
    "    ambient = 0\n",
    "    alpha = 1\n",
    "    seeds = [10, 20, 30, 40, 50]\n",
    "    \n",
    "    dataset_print, dataset, K, hidden = dataset_and_K(dataset, season, K, p, ERO_style, eta)\n",
    "\n",
    "    if dataset[:3] != 'ERO':\n",
    "        num_trials = 10\n",
    "        AllTrain = True\n",
    "        train_ratio = 1\n",
    "        test_ratio = 1\n",
    "        seeds = [10]\n",
    "    \n",
    "\n",
    "    kendalltau_res_full, final_upset_full, selected_indices = GNN_selection(dataset, K, train_ratio, test_ratio, AllTrain, hidden, num_trials)\n",
    "    dir_name = '../result_arrays/'+dataset\n",
    "    kendalltau_res_all = np.zeros((METRICS_NUM, METHODS_NUM, num_trials*len(seeds)))\n",
    "    kendalltau_res_all[:] = np.nan\n",
    "    final_upset_all = np.zeros((METRICS_NUM, METHODS_NUM, num_trials*len(seeds), NUM_UPSET_CHOICES))\n",
    "    final_upset_all[:] = np.nan\n",
    "    for i in range(METRICS_NUM):\n",
    "        compare_names_all = ['Non_proximal', 'Proximal']\n",
    "        if i == 0 and dataset[:3] != 'ERO':\n",
    "            continue\n",
    "        kendalltau_res = kendalltau_res_full[i]\n",
    "        final_upset = final_upset_full[i]\n",
    "\n",
    "        # try to include mvr results\n",
    "        save_name = generate_save_name(dataset=dataset, all_methods=mvr, K=K, num_trials=num_trials, \n",
    "                                       train_ratio=train_ratio, test_ratio=test_ratio,  AllTrain=AllTrain)\n",
    "        method_str, _ = generate_method_str_and_compare_names_all(mvr)\n",
    "        if os.path.exists(os.path.join(dir_name,'kendalltau',method_str,save_name) + '.npy'):\n",
    "            kendalltau_res = np.concatenate((np.load(os.path.join(dir_name,'kendalltau',method_str,save_name) + '.npy')[:, :, 2, 0], kendalltau_res), axis=0)\n",
    "            final_upset = np.concatenate((np.load(os.path.join(dir_name,'upset',method_str,save_name) + '.npy'), final_upset), axis=0)\n",
    "        else:\n",
    "            fill_nan = np.zeros((1, num_trials*len(seeds)))\n",
    "            fill_nan[:] = np.nan\n",
    "            kendalltau_res = np.concatenate((fill_nan, kendalltau_res), axis=0)\n",
    "            fill_nan = np.zeros((1, num_trials*len(seeds), NUM_UPSET_CHOICES))\n",
    "            fill_nan[:] = np.nan\n",
    "            final_upset = np.concatenate((fill_nan, final_upset), axis=0)\n",
    "        compare_names_all = ['mvr'] + compare_names_all\n",
    "\n",
    "\n",
    "        # include baseline results\n",
    "        save_name = generate_save_name(dataset=dataset, all_methods=baselines, K=K, num_trials=num_trials, \n",
    "                                       train_ratio=train_ratio, test_ratio=test_ratio,  AllTrain=AllTrain)\n",
    "        method_str, compare_names_baselines = generate_method_str_and_compare_names_all(baselines)\n",
    "        kendalltau_res = np.concatenate((np.load(os.path.join(dir_name,'kendalltau',method_str,save_name) + '.npy')[:, :, 2, 0], kendalltau_res), axis=0)\n",
    "        final_upset = np.concatenate((np.load(os.path.join(dir_name,'upset',method_str,save_name) + '.npy'), final_upset), axis=0)\n",
    "        compare_names_all = compare_names_baselines + compare_names_all\n",
    "\n",
    "        kendalltau_res_all[i] = kendalltau_res\n",
    "        final_upset_all[i] = final_upset\n",
    "        \n",
    "    return dataset_print, kendalltau_res_all, final_upset_all, selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GNN_load_results(dataset=dataset, all_methods=all_GNNs, K=5, train_with='dist', \n",
    "                                   upset_ratio_coeff=1, upset_margin_coeff=0,  upset_margin=0.01,\n",
    "                                    trainable_alpha=False, lr=0.01, hidden=32, num_trials=10, \n",
    "                                   train_ratio=0.8, test_ratio=0.1,  AllTrain=True, rank_baseline='SpringRank', \n",
    "                                   sigma=1, Fiedler_layer_num=5, pretrain_epochs=50, pretrain_with='dist'):\n",
    "    save_name = generate_save_name(dataset=dataset, all_methods=all_GNNs, K=K, train_with=train_with, \n",
    "                                   upset_ratio_coeff=upset_ratio_coeff, upset_margin_coeff=upset_margin_coeff, upset_margin=upset_margin,\n",
    "                                    trainable_alpha=trainable_alpha, lr=lr, hidden=hidden, num_trials=num_trials, \n",
    "                                   train_ratio=train_ratio, test_ratio=test_ratio,  AllTrain=AllTrain, rank_baseline=rank_baseline, \n",
    "                                   sigma=sigma, Fiedler_layer_num=Fiedler_layer_num, pretrain_epochs=pretrain_epochs, pretrain_with=pretrain_with)\n",
    "    method_str, _ = generate_method_str_and_compare_names_all(all_GNNs)\n",
    "    dir_name = '../result_arrays/'+dataset\n",
    "    kendalltau_res = None\n",
    "    if dataset[:3] == 'ERO':\n",
    "        kendalltau_res = np.load(os.path.join(dir_name,'kendalltau',method_str,save_name) + '.npy')[:, :, 2, 0]\n",
    "    final_upset = np.load(os.path.join(dir_name,'upset',method_str,save_name) + '.npy')\n",
    "    return kendalltau_res, final_upset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b38600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GNN_selection(dataset, K, train_ratio, test_ratio, AllTrain, hidden, num_trials):\n",
    "    upset_margin = 0.01\n",
    "    pretrain_epochs = 50\n",
    "    sigma = 1\n",
    "    full_results_non_proximal = 1000*np.ones((METRICS_NUM, NON_PROXIMAL_GNN_NUM * non_proximal_cases_num))\n",
    "    full_results_non_proximal[0] = 0\n",
    "    full_results_proximal = 1000*np.ones((METRICS_NUM, PROXIMAL_GNN_NUM * proximal_cases_num))\n",
    "    full_results_proximal[0] = 0\n",
    "    final_ind_non_proximal = 0\n",
    "    final_ind_proximal = 0\n",
    "    has_result = False\n",
    "    for lr_ind, lr in enumerate(lr_list):\n",
    "        for train_ind, train_with in enumerate(train_with_list):\n",
    "            for pretrain_ind, pretrain_with in enumerate(pretrain_with_list_dict[train_with]):\n",
    "                for margin_coeff_ind, upset_margin_coeff in enumerate(upset_margin_coeff_list):\n",
    "                    for trainable_alpha_ind, trainable_alpha in enumerate(trainable_alpha_list):\n",
    "                        for base_ind, rank_baseline in enumerate(rank_baseline_list):\n",
    "                            for ratio_coeff_ind, upset_ratio_coeff in enumerate(upset_ratio_coeff_list):\n",
    "                                for Fiedler_layer_num_ind, Fiedler_layer_num in enumerate(Fiedler_layer_num_list):\n",
    "                                    for pretrain_epochs_ind, pretrain_epochs in enumerate(pretrain_epochs_list):\n",
    "                                        try:\n",
    "                                            kendalltau, upsets = GNN_load_results(dataset=dataset, all_methods=all_GNNs, K=K, train_with=train_with, \n",
    "                                                    upset_ratio_coeff=upset_ratio_coeff, upset_margin_coeff=upset_margin_coeff, upset_margin=upset_margin,\n",
    "                                                    trainable_alpha=trainable_alpha, lr=lr, hidden=hidden, num_trials=num_trials, \n",
    "                                                    train_ratio=train_ratio, test_ratio=test_ratio,  AllTrain=AllTrain, rank_baseline=rank_baseline, \n",
    "                                                    sigma=sigma, Fiedler_layer_num=Fiedler_layer_num, pretrain_epochs=pretrain_epochs, pretrain_with=pretrain_with)\n",
    "                                            if kendalltau is not None:\n",
    "                                                mean_kendalltau = np.nanmean(kendalltau[non_proximal_bool], axis=1)\n",
    "                                                full_results_non_proximal[0, final_ind_non_proximal: final_ind_non_proximal + NON_PROXIMAL_GNN_NUM] = mean_kendalltau\n",
    "                                                mean_kendalltau = np.nanmean(kendalltau[proximal_bool], axis=1)\n",
    "                                                full_results_proximal[0, final_ind_proximal: final_ind_proximal + PROXIMAL_GNN_NUM] = mean_kendalltau\n",
    "\n",
    "                                            mean_upsets = upsets[non_proximal_bool].swapaxes(0,-1).mean(axis=1)\n",
    "                                            full_results_non_proximal[1:, final_ind_non_proximal: final_ind_non_proximal + NON_PROXIMAL_GNN_NUM] = mean_upsets\n",
    "                                            mean_upsets = upsets[proximal_bool].swapaxes(0,-1).mean(axis=1)\n",
    "                                            full_results_proximal[1:, final_ind_proximal: final_ind_proximal + PROXIMAL_GNN_NUM] = mean_upsets\n",
    "                                            has_result = True\n",
    "                                        except FileNotFoundError:\n",
    "                                            pass\n",
    "                                        except AssertionError:\n",
    "                                            pass\n",
    "                                        except ValueError:\n",
    "                                            pass\n",
    "                                        final_ind_non_proximal += NON_PROXIMAL_GNN_NUM\n",
    "                                        final_ind_proximal += PROXIMAL_GNN_NUM\n",
    "    if has_result:\n",
    "        best_ind = np.zeros((METRICS_NUM, 2))\n",
    "        best_vals = np.zeros((METRICS_NUM, 2))\n",
    "        best_vals[:] = np.nan\n",
    "        if dataset[:3] == 'ERO':\n",
    "            full_results_non_proximal[0] = np.nan_to_num(full_results_non_proximal[0], nan=0)\n",
    "            best_ind[0, 0] = full_results_non_proximal[0].argmax()\n",
    "            best_vals[0, 0] = np.nanmax(full_results_non_proximal[0])\n",
    "            full_results_proximal[0] = np.nan_to_num(full_results_proximal[0], nan=0)\n",
    "            best_ind[0, 1] = full_results_proximal[0].argmax()\n",
    "            best_vals[0, 1] = np.nanmax(full_results_proximal[0])\n",
    "        full_results_non_proximal[1:] = np.nan_to_num(full_results_non_proximal[1:], nan=1000)\n",
    "        full_results_proximal[1:] = np.nan_to_num(full_results_proximal[1:], nan=1000)\n",
    "        \n",
    "        for upset_ind in range(NUM_UPSET_CHOICES):\n",
    "            best_vals[1+upset_ind, 0] = np.nanmin(full_results_non_proximal[1+upset_ind])\n",
    "            best_ind[1+upset_ind, 0] = full_results_non_proximal[1+upset_ind].argmin()\n",
    "            best_vals[1+upset_ind, 1] = np.nanmin(full_results_proximal[1+upset_ind])\n",
    "            best_ind[1+upset_ind, 1] = full_results_proximal[1+upset_ind].argmin()\n",
    "        selected_indices = np.zeros((METRICS_NUM, 2, GNN_CHOICES_NUM+1))\n",
    "        kendalltau_res = np.zeros((METRICS_NUM, 2, 10))\n",
    "        kendalltau_res[:] = np.nan\n",
    "        final_upset = np.zeros((METRICS_NUM, 2, 10, NUM_UPSET_CHOICES)) # the first \"2\" means non-proximal and proximal\n",
    "        final_upset[:] = np.nan\n",
    "        for i in range(METRICS_NUM):\n",
    "            if i == 0 and dataset[:3] != 'ERO':\n",
    "                continue\n",
    "            for j, proximal_or_not in enumerate(['non_proximal', 'proximal']):\n",
    "                if j == 0:\n",
    "                    selected_indices[i, j] = non_proximal_ind_correspondence_dict[best_ind[i, j]]\n",
    "                else:\n",
    "                    selected_indices[i, j] = proximal_ind_correspondence_dict[best_ind[i, j]]\n",
    "                \n",
    "                lr = lr_list[int(selected_indices[i, j, 0])]\n",
    "                train_with = train_with_list[int(selected_indices[i, j, 1])]\n",
    "                pretrain_with = pretrain_with_list_dict[train_with][int(selected_indices[i, j, 2])]\n",
    "                upset_margin_coeff = upset_margin_coeff_list[int(selected_indices[i, j, 3])]\n",
    "                trainable_alpha = trainable_alpha_list[int(selected_indices[i, j, 4])]\n",
    "                rank_baseline = rank_baseline_list[int(selected_indices[i, j, 5])]\n",
    "                upset_ratio_coeff = upset_ratio_coeff_list[int(selected_indices[i, j, 6])]\n",
    "                Fiedler_layer_num = Fiedler_layer_num_list[int(selected_indices[i, j, 7])]\n",
    "                pretrain_epochs = pretrain_epochs_list[int(selected_indices[i, j, 8])]\n",
    "\n",
    "                sel_ind = int(selected_indices[i, j, -1])\n",
    "                if j == 0:\n",
    "                    GNN_selected = GNN_names_non_proximal[sel_ind]\n",
    "                else:\n",
    "                    GNN_selected = GNN_names_proximal[sel_ind]\n",
    "                selected_vals = [lr, train_with, pretrain_with, upset_margin_coeff, \\\n",
    "                                  trainable_alpha, \\\n",
    "                                 rank_baseline, upset_ratio_coeff, Fiedler_layer_num, pretrain_epochs]\n",
    "                print_str = GNN_selected + ' on ' + dataset + ' among ' + proximal_or_not + ': '\n",
    "                for k in range(GNN_CHOICES_NUM):\n",
    "                    print_str += GNN_selection_choices[k] + '=' + str(selected_vals[k]) + ', '\n",
    "                print_str += 'for the best ' + selected_metrics[i]\n",
    "                kendalltau, upsets = GNN_load_results(dataset=dataset, all_methods=all_GNNs, K=K, train_with=train_with, \n",
    "                                           upset_ratio_coeff=upset_ratio_coeff, upset_margin_coeff=upset_margin_coeff, upset_margin=upset_margin,\n",
    "                                            trainable_alpha=trainable_alpha, lr=lr, hidden=hidden, num_trials=num_trials, \n",
    "                                           train_ratio=train_ratio, test_ratio=test_ratio,  AllTrain=AllTrain, rank_baseline=rank_baseline, \n",
    "                                           sigma=sigma, Fiedler_layer_num=Fiedler_layer_num, pretrain_epochs=pretrain_epochs, pretrain_with=pretrain_with)\n",
    "                if j == 0:\n",
    "                    if kendalltau is not None:\n",
    "                        kendalltau_res[i, j] = (kendalltau[non_proximal_bool])[sel_ind]\n",
    "                    upsets_res = (upsets[non_proximal_bool])[sel_ind]\n",
    "                else:\n",
    "                    if kendalltau is not None:\n",
    "                        kendalltau_res[i, j] = (kendalltau[proximal_bool])[sel_ind]\n",
    "                    upsets_res = (upsets[proximal_bool])[sel_ind]\n",
    "                final_upset[i, j] = np.array(upsets_res)\n",
    "        return kendalltau_res, final_upset, selected_indices\n",
    "    else:\n",
    "        raise FileNotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37391c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_results(dataset_list=['HeadToHead', 'finance','animal', 'faculty_business', \\\n",
    "                                   'faculty_cs', 'faculty_history', \\\n",
    "                                   'avg_basketball', 'avg_finer_basketball', 'avg_football', 'avg_finer_football', \\\n",
    "                                  'basketball', 'finer_basketball', 'football', 'finer_football'], selected_printing=False):\n",
    "    dataset_name_full = []\n",
    "    kendalltau_res_all_full = []\n",
    "    final_upset_all_full = []\n",
    "    for dataset in dataset_list:\n",
    "        if dataset[:4] == 'avg_':\n",
    "            kendalltau_res_all_full_avg = []\n",
    "            final_upset_all_full_avg = []\n",
    "            for season in season_list_dict[dataset[4:]]:\n",
    "                _, kendalltau_res_all, final_upset_all, _ = extract_results(dataset=dataset[4:], \n",
    "                                                        season=season)\n",
    "                kendalltau_res_all_full_avg.append(kendalltau_res_all)\n",
    "                final_upset_all_full_avg.append(final_upset_all)\n",
    "            dataset_name_full.append(name_mapping_dict[dataset])\n",
    "            kendalltau_res_all_full.append(np.nanmean(np.array(kendalltau_res_all_full_avg), axis=0))\n",
    "            final_upset_all_full.append(np.nanmean(np.array(final_upset_all_full_avg), axis=0))\n",
    "        else:\n",
    "            for p in p_list_dict[dataset]:\n",
    "                for K in K_list_dict[dataset]:\n",
    "                    for eta in eta_list_dict[dataset]:\n",
    "                        for ERO_style in ERO_style_list_dict[dataset]:\n",
    "                            for season in season_list_dict[dataset]:\n",
    "                                if selected_printing:\n",
    "                                    if ERO_style == 'uniform':\n",
    "                                        if p == 0.05 and eta not in [0.1, 0.3]:\n",
    "                                            continue\n",
    "                                        elif p == 1 and eta not in [0.5, 0.7]:\n",
    "                                            continue\n",
    "                                    else:\n",
    "                                        if p == 0.05 and eta not in [0.2, 0.4]:\n",
    "                                            continue\n",
    "                                        elif p == 1 and eta not in [0.6]:\n",
    "                                            continue\n",
    "                                try:\n",
    "                                    dataset_long, kendalltau_res_all, final_upset_all, _ = extract_results(dataset=dataset, \n",
    "                                                    season=season, p=p, K=K, eta=eta,\n",
    "                                                    ERO_style=ERO_style)\n",
    "                                    dataset_name_full.append(dataset_long)\n",
    "                                    kendalltau_res_all_full.append(kendalltau_res_all)\n",
    "                                    final_upset_all_full.append(final_upset_all)\n",
    "                                except FileNotFoundError:\n",
    "                                    print('No result yet for {}, season {}, p={}, K={}, eta={}, ERO style = {}.'.format(dataset,\n",
    "                                        season, p, K, eta, ERO_style))\n",
    "    \n",
    "\n",
    "    full_results = np.concatenate((np.expand_dims(np.array(kendalltau_res_all_full), axis=-1), np.array(final_upset_all_full)), axis=-1)\n",
    "    for i in range(METRICS_NUM):\n",
    "        for j in range(1, METRICS_NUM):\n",
    "            results_to_print = full_results[:,j,:,:,i].swapaxes(0,2)\n",
    "            if not np.isnan(results_to_print).all() and (i==j or i==0):\n",
    "                dataset_name_print = dataset_name_full\n",
    "                compare_names_print = compare_names_all\n",
    "                if i not in [1, 3]:\n",
    "                    mvr_ind = compare_names_all.index('mvr')\n",
    "                    results_to_print = np.delete(results_to_print, mvr_ind, axis=1)\n",
    "                    compare_names_print = np.delete(compare_names_print, mvr_ind, axis=0)\n",
    "                if selected_printing:\n",
    "                    results_to_print_mean = np.nanmean(results_to_print, axis=0)\n",
    "                    if i == 0: # the bigger, the better\n",
    "                        thres =  -np.sort(-results_to_print_mean, axis=0)[1]\n",
    "                        bool_vals = np.bitwise_and(results_to_print_mean[-1]>=thres, results_to_print_mean[-1] >= results_to_print_mean[-2])\n",
    "                        bool_vals = np.bitwise_and(results_to_print_mean[-1] >= results_to_print_mean[-2], results_to_print_mean[-1] >= results_to_print_mean[0])\n",
    "                        selected_ind = np.arange(results_to_print.shape[-1])[bool_vals]\n",
    "                    else:\n",
    "                        thres = np.sort(results_to_print_mean, axis=0)[1]\n",
    "                        bool_vals = np.bitwise_and(results_to_print_mean[-1]<=thres, results_to_print_mean[-1] <= results_to_print_mean[-2])\n",
    "                        bool_vals = np.bitwise_and(results_to_print_mean[-1] <= results_to_print_mean[-2], results_to_print_mean[-1] <= results_to_print_mean[0])\n",
    "                        selected_ind = np.arange(results_to_print.shape[-1])[bool_vals]\n",
    "                    results_to_print = np.take(results_to_print, selected_ind, axis=-1)\n",
    "                    dataset_name_print = np.take(dataset_name_print, selected_ind)\n",
    "                title_name = selected_metrics[i] + ' with best ' + selected_metrics[j]\n",
    "                print_overall_performance_mean_std(title_name, results_to_print, \n",
    "                                compare_names_print, dataset_name_print, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63983dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis_results(['ERO'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results(['ERO'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40af519b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_models(dataset_list=['HeadToHead', 'finance','animal', 'faculty_business', 'faculty_cs', 'faculty_history', 'basketball', 'finer_basketball', 'football', 'finer_football']):\n",
    "    dataset_name_full = []\n",
    "    kendalltau_res_all_full = []\n",
    "    final_upset_all_full = []\n",
    "    selected_indices_full = []\n",
    "    for dataset in dataset_list:\n",
    "        for p in p_list_dict[dataset]:\n",
    "            for K in K_list_dict[dataset]:\n",
    "                for eta in eta_list_dict[dataset]:\n",
    "                    for ERO_style in ERO_style_list_dict[dataset]:\n",
    "                        for season in season_list_dict[dataset]:\n",
    "                            try:\n",
    "                                dataset_long, kendalltau_res_all, final_upset_all, selected_indices = extract_results(dataset=dataset, \n",
    "                                                season=season, p=p, K=K, eta=eta,\n",
    "                                                ERO_style=ERO_style)\n",
    "                                dataset_name_full.append(dataset_long)\n",
    "                                kendalltau_res_all_full.append(kendalltau_res_all)\n",
    "                                final_upset_all_full.append(final_upset_all)\n",
    "                                selected_indices_full.append(selected_indices)\n",
    "                            except FileNotFoundError:\n",
    "                                print('No result yet for {}, season {}, p={}, K={}, eta={}, ERO style = {}.'.format(dataset,\n",
    "                                    season, p, K, eta, ERO_style))\n",
    "    \n",
    "    full_results = np.concatenate((np.expand_dims(np.array(kendalltau_res_all_full), axis=-1), np.array(final_upset_all_full)), axis=-1)\n",
    "    for i in range(METRICS_NUM):\n",
    "        for j, proximal_or_not in enumerate(['non_proximal', 'proximal']):\n",
    "            title = 'GNN selection among {} for the best {}.'.format(proximal_or_not, selected_metrics[i])\n",
    "            label = 'GNN_selection_'+proximal_or_not+'_'+selected_metrics[i]\n",
    "            print(title)\n",
    "            t = Texttable(max_width=120)\n",
    "            t.set_deco(Texttable.HEADER)\n",
    "            t.add_row(['Data set', 'Variant'] + GNN_selection_choices_curr)\n",
    "            for dataset_ind, dataset in enumerate(dataset_name_full):\n",
    "                if i == 0 and dataset[:3] != 'ERO':\n",
    "                    continue\n",
    "                selected_indices = selected_indices_full[dataset_ind]\n",
    "\n",
    "                lr = lr_list[int(selected_indices[i, j, 0])]\n",
    "                train_with = train_with_list[int(selected_indices[i, j, 1])]\n",
    "                pretrain_with = name_mapping_dict[pretrain_with_list_dict[train_with][int(selected_indices[i, j, 2])]]\n",
    "                train_with = name_mapping_dict[train_with]\n",
    "                upset_margin_coeff = upset_margin_coeff_list[int(selected_indices[i, j, 3])]\n",
    "                trainable_alpha = str(trainable_alpha_list[int(selected_indices[i, j, 4])])\n",
    "                rank_baseline = name_mapping_dict[rank_baseline_list[int(selected_indices[i, j, 5])]]\n",
    "                upset_ratio_coeff = upset_ratio_coeff_list[int(selected_indices[i, j, 6])]\n",
    "                Fiedler_layer_num = Fiedler_layer_num_list[int(selected_indices[i, j, 7])]\n",
    "                pretrain_epochs = pretrain_epochs_list[int(selected_indices[i, j, 8])]\n",
    "                if train_with[:8] != 'proximal':\n",
    "                    pretrain_with = '--'\n",
    "                    trainable_alpha = '--'\n",
    "                    pretrain_epochs = '--'\n",
    "                \n",
    "                \n",
    "\n",
    "                sel_ind = int(selected_indices[i, j, -1])\n",
    "                if j == 0:\n",
    "                    GNN_selected = GNN_names_non_proximal[sel_ind]\n",
    "                else:\n",
    "                    GNN_selected = GNN_names_proximal[sel_ind]\n",
    "                if train_with[:8] != 'proximal' and GNN_selected not in GNN_names_proximal:\n",
    "                    Fiedler_layer_num = '--'\n",
    "                GNN_selected = name_mapping_dict[GNN_selected]\n",
    "                \n",
    "                if train_with != 'proximal baseline' and GNN_selected not in ['DIGRAC proximal baseline', 'ib proximal baseline']:\n",
    "                    rank_baseline = '--'\n",
    "\n",
    "\n",
    "                selected_vals = [train_with, pretrain_with, upset_margin_coeff, \\\n",
    "                                 rank_baseline, upset_ratio_coeff]\n",
    "                t.add_row([dataset, GNN_selected] + selected_vals)\n",
    "            print(t.draw())\n",
    "            print(latextable.draw_latex(t, caption=title, label=\"table:\"+label) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e75118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_models(['ERO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb996658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('GNNRank': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "541ec6e12d81ce70db7e175c19aab1e0eef576997373747c03414131db6775d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
